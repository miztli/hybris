# -----------------------------------------------------------------------
# [y] hybris Platform
#
# Copyright (c) 2018 SAP SE or an SAP affiliate company. All rights reserved.
#
# This software is the confidential and proprietary information of SAP
# ("Confidential Information"). You shall not disclose such Confidential
# Information and shall use it only in accordance with the terms of the
# license agreement you entered into with SAP.
# -----------------------------------------------------------------------

# Specifies the location of the spring context file putted automatically to the global platform application context.
processing.application-context=processing-spring.xml,cronjob-spring.xml,task-spring.xml,processengine-spring.xml,processing-jmx-spring.xml,processing-distributed-spring.xml

processing.tomcat.tld.scan=displaytag*.jar,javax.servlet.jsp.jstl-*.jar
processing.tomcat.tld.default.scan.enabled=false
processing.tomcat.pluggability.scan=displaytag*.jar,javax.servlet.jsp.jstl-*.jar
processing.tomcat.pluggability.default.scan.enabled=false

# -----------------------------------------------------------------------
# Node configuration
# -----------------------------------------------------------------------

# Allows to exclude a set of cluster nodes from running the task engine.
# Even without enabled engine it is possible to schedule new tasks on such
# nodes. The only difference is that they must be performed by other nodes. 

#task.excluded.cluster.ids=1,4,12

# -----------------------------------------------------------------------
# Worker configuration
# -----------------------------------------------------------------------

# The maximum number of workers to be used for performing tasks. 
task.workers.max=10

# The time to keep a idle worker alive before removing it from pool.
task.workers.idle=20

# -----------------------------------------------------------------------
# Polling configuration
# -----------------------------------------------------------------------

# The time between two attempts for getting new tasks to perform.
task.polling.interval=10

#the amount of time (in seconds) waiting for completion of all tasks during shutdown process (see DefaultTaskService.destroy()) 
tasks.shutdown.wait=15

#enables task processing, 'true' by default, setting it to 'false' disables it (tasks and cronjobs will not be executed on the current node)
task.engine.loadonstartup=true

#allows to specify how many tasks are queried. #workers variable can be used to get number of current worker threads 
task.engine.query.tasks.count=#workers * 200

#allows to specify how many conditions are queried. #workers variable can be used to get number of current worker threads
task.engine.query.conditions.count=#workers * 200

# defines the time threshold differentiating the current tasks and old tasks.
# If greater than 0 and task.engine.query.full.interval property is set, the old tasks will be checked less often than current tasks.
# If value <= 0 then all tasks will be checked whether they can be processed
#
# Value in number of hours.
task.engine.query.full.executiontime.threshold=0

# defines the interval of retrieving old and expired tasks to process. If greater than 0, the old and expired tasks
# will be checked every n-th poll, where n is the value of this property. If equal to 0, old and expired tasks will be checked
# with every poll
task.engine.query.full.interval=0

# allows to specify (in seconds) the minimal interval for polling the database for new tasks;
# if the polling is executed more frequently, the tasks buffered from last poll will be used until the interval is assured
task.polling.interval.min=10

# allows you to specify (in seconds) the interval of count queries executed to report statistics of tasks to be executed
# default value is -1 which means no count queries is executed
# value 0 means that query will be executed with the polling query  
task.queue.size.reporting.interval=60

# If set to true forces task engine to process only task assigned to the same node on which task engine is running.
# Default value is false
task.engine.exclusive.mode=false

# Set maximum numer of attempts when trying to recover from taskengine startup errors
task.engine.retry.maxattempts=5 

# Set the backoff period for retry in milliseconds
task.engine.retry.backoffperiod=2000


# defines the interval (in seconds) how often the scheduler should try to fill the tasks' queue
task.auxiliaryTables.scheduler.interval.seconds=10

# defines the threshold (in seconds) of tasks' waiting time in queue. The tasks that has not been processed within this time
# will be removed from queue (if the task is still present in tasks table, it will be rescheduled next time)
task.auxiliaryTables.scheduler.cleanQueue.oldTasksThreshold.seconds=900

# defines the interval (in seconds) at which each node should update (or insert if not existing) it's heartbeat in auxiliary
# table. The heartbeat is used to determine whether the node is still online and working
task.auxiliaryTables.worker.registration.interval.seconds=5

# defines the interval since current time in which the node's heartbeat is treated as valid
task.auxiliaryTables.worker.activation.interval.seconds=10

# defines the interval after which the node will be deactivated. If node doesn't update the heartbeat during this interval
# it will not be taken into account when scheduling takes place
task.auxiliaryTables.worker.deactivation.interval.seconds=30

# defines the interval after which the node will be removed. If node # doesn't update it's heartbeat during this interval
# all tasks assigned to this node will be unassigned
task.auxiliaryTables.worker.removal.interval.seconds=150

# the multiplier of number of tasks that should be available in the queue for each node. The base value is equal
# tasks.workers.max * 2 and then multiplied by the value of this property. If the number of tasks available in queue for any
# node is less then the calculated amount, then the scheduler will try to move all valid tasks to queue
task.auxiliaryTables.worker.lowTasksCountThreshold.multiplier=20

# max number of tasks that should be batch-removed from queue
task.auxiliaryTables.worker.deleteTasks.maxBatchSize=100

# the lowest value of range (inclusive) assigned to tasks in queue
task.auxiliaryTables.tasks.range.start=0

# the highest value of range (inclusive) assigned to tasks in queue
task.auxiliaryTables.tasks.range.end=1000

# number of seconds that task in queue should be locked for processing by other nodes
task.auxiliaryTables.tasks.lockDuration.seconds=20


#processengine
processengine.process.log.pattern=%-5p %X{RemoteAddr}%X{Tenant}[%c{1}] %m%n

# If set to true, process engine will set attribute canjoinpreviousnode for action node/script node to true 
# if this attribute isn't present.
# Default value is false
processengine.process.canjoinpreviousnode.default=false

# If set to true, task engine tasks' logger will persist the log file in database. The log file should contain logs
# gathered while processing the task by task engine
# Default value is true
processengine.process.log.dbstore.enabled=true

# -----------------------------------------------------------------------
# Distributed Processes internal settings
# -----------------------------------------------------------------------
#
# Since distributed processes run fully asynchronous there can be a small delay between 
# status changes become visible in both process and task item (due to cache invalidation 
# being iterative). To mitigate a wait delay can be defined in milliseconds:    
#
distributed.processes.transitions.stale.status.max.wait=10000

#
# Switching distributed process status may require several attempts due to its asynchronous 
# nature. This setting defines the maximum allowed number of retries.
#
distributed.processes.transitions.max.retries=5

# -----------------------------------------------------------------------
# Simple Template for Distributed Processes settings
# -----------------------------------------------------------------------

distributed.process.simple.template.max.batch.retries=3
distributed.process.simple.template.batch.size=100

keygen.distributed.process.code.name=ext.processing
keygen.distributed.process.code.digits=8
keygen.distributed.process.code.start=00000000
keygen.distributed.process.code.type=alphanumeric
keygen.distributed.process.code.template=$


# secure media folder
media.folder.cronjob.secured=true

# Trigger task runner settings
# set maximum number of attempts when unexpected exception happen during execution of job
taskRunner.trigger.retry.maxattempts=10
# Defines the interval in seconds after which failed job will be retried. 
# It only applies if next time job execution has not been calculated
taskRunner.trigger.retry.interval.seconds=300